{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 64                            # 배치 크기\n",
    "epochs = 100                            # 학습 epochs\n",
    "latent_dim = 256                        # 단어 인코딩 축소 차원 \n",
    "n_max_sample = 10000                    # 학습 시킬 최대 샘플 수\n",
    "data_path = './data/eng-fra/fra.txt'    # 데이터 텍스트 파일 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
       " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n",
       " 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)',\n",
       " 'Run!\\tCours\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)',\n",
       " 'Run!\\tCourez\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)',\n",
       " 'Who?\\tQui ?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)',\n",
       " 'Wow!\\tÇa alors\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)',\n",
       " 'Fire!\\tAu feu !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)',\n",
       " \"Help!\\tÀ l'aide\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\",\n",
       " 'Jump.\\tSaute.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Micsmithel)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인풋, 타겟 텍스트 데이터 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_txts = []\n",
    "y_txts = []\n",
    "x_chars_uni = set()\n",
    "y_chars_uni = set()\n",
    "n_sample = min(n_max_sample, len(lines) - 1)    \n",
    "\n",
    "for line in lines[:n_sample]:\n",
    "    x_txt, y_txt, _ = line.split('\\t')\n",
    "    y_txt = '\\t' + y_txt + '\\n'\n",
    "    x_txts.append(x_txt)\n",
    "    y_txts.append(y_txt)\n",
    "    \n",
    "    for char in x_txt:\n",
    "        if char not in x_chars_uni:\n",
    "            x_chars_uni.add(char)\n",
    "    for char in y_txt:\n",
    "        if char not in y_chars_uni:\n",
    "            y_chars_uni.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.', 'Hi.', 'Hi.', 'Run!', 'Run!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_txts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tVa !\\n',\n",
       " '\\tSalut !\\n',\n",
       " '\\tSalut.\\n',\n",
       " '\\tCours\\u202f!\\n',\n",
       " '\\tCourez\\u202f!\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_txts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'é'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_chars_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\xa0',\n",
       " '«',\n",
       " '»',\n",
       " 'À',\n",
       " 'Ç',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ô',\n",
       " 'ù',\n",
       " 'û',\n",
       " 'œ',\n",
       " '\\u2009',\n",
       " '’',\n",
       " '\\u202f'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_chars_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 단위 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_chars_uni = sorted(list(x_chars_uni))\n",
    "y_chars_uni = sorted(list(y_chars_uni))\n",
    "n_encoder_tokens = len(x_chars_uni)\n",
    "n_decoder_tokens = len(y_chars_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니크 인코더 토큰 글자 수:  71\n",
      "유니크 디코더 토큰 글자 수:  93\n"
     ]
    }
   ],
   "source": [
    "print(\"유니크 인코더 토큰 글자 수: \", n_encoder_tokens)\n",
    "print(\"유니크 디코더 토큰 글자 수: \", n_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 문장내 최대 문자 수:  15\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_len = 0\n",
    "for txt in x_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_encoder_seq_len = max(txt_len, \n",
    "                              max_encoder_seq_len)\n",
    "print(\"인코더 문장내 최대 문자 수: \", max_encoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코더 문장내 최대 문자 수:  59\n"
     ]
    }
   ],
   "source": [
    "max_decoder_seq_len = 0\n",
    "for txt in y_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_decoder_seq_len = max(txt_len, \n",
    "                              max_decoder_seq_len)\n",
    "print(\"디코더 문장내 최대 문자 수: \", max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 별 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_token_idx = {}\n",
    "for idx, char in enumerate(x_chars_uni):\n",
    "    x_token_idx[char] = idx\n",
    "    \n",
    "y_token_idx ={}\n",
    "for idx, char in enumerate(y_chars_uni):\n",
    "    y_token_idx[char] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '$': 2,\n",
       " '%': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " '?': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'Y': 43,\n",
       " 'a': 44,\n",
       " 'b': 45,\n",
       " 'c': 46,\n",
       " 'd': 47,\n",
       " 'e': 48,\n",
       " 'f': 49,\n",
       " 'g': 50,\n",
       " 'h': 51,\n",
       " 'i': 52,\n",
       " 'j': 53,\n",
       " 'k': 54,\n",
       " 'l': 55,\n",
       " 'm': 56,\n",
       " 'n': 57,\n",
       " 'o': 58,\n",
       " 'p': 59,\n",
       " 'q': 60,\n",
       " 'r': 61,\n",
       " 's': 62,\n",
       " 't': 63,\n",
       " 'u': 64,\n",
       " 'v': 65,\n",
       " 'w': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69,\n",
       " 'é': 70}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 0,\n",
       " '\\n': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " '3': 16,\n",
       " '5': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " ':': 20,\n",
       " '?': 21,\n",
       " 'A': 22,\n",
       " 'B': 23,\n",
       " 'C': 24,\n",
       " 'D': 25,\n",
       " 'E': 26,\n",
       " 'F': 27,\n",
       " 'G': 28,\n",
       " 'H': 29,\n",
       " 'I': 30,\n",
       " 'J': 31,\n",
       " 'K': 32,\n",
       " 'L': 33,\n",
       " 'M': 34,\n",
       " 'N': 35,\n",
       " 'O': 36,\n",
       " 'P': 37,\n",
       " 'Q': 38,\n",
       " 'R': 39,\n",
       " 'S': 40,\n",
       " 'T': 41,\n",
       " 'U': 42,\n",
       " 'V': 43,\n",
       " 'Y': 44,\n",
       " 'a': 45,\n",
       " 'b': 46,\n",
       " 'c': 47,\n",
       " 'd': 48,\n",
       " 'e': 49,\n",
       " 'f': 50,\n",
       " 'g': 51,\n",
       " 'h': 52,\n",
       " 'i': 53,\n",
       " 'j': 54,\n",
       " 'k': 55,\n",
       " 'l': 56,\n",
       " 'm': 57,\n",
       " 'n': 58,\n",
       " 'o': 59,\n",
       " 'p': 60,\n",
       " 'q': 61,\n",
       " 'r': 62,\n",
       " 's': 63,\n",
       " 't': 64,\n",
       " 'u': 65,\n",
       " 'v': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69,\n",
       " '\\xa0': 70,\n",
       " '«': 71,\n",
       " '»': 72,\n",
       " 'À': 73,\n",
       " 'Ç': 74,\n",
       " 'É': 75,\n",
       " 'Ê': 76,\n",
       " 'à': 77,\n",
       " 'â': 78,\n",
       " 'ç': 79,\n",
       " 'è': 80,\n",
       " 'é': 81,\n",
       " 'ê': 82,\n",
       " 'ë': 83,\n",
       " 'î': 84,\n",
       " 'ï': 85,\n",
       " 'ô': 86,\n",
       " 'ù': 87,\n",
       " 'û': 88,\n",
       " 'œ': 89,\n",
       " '\\u2009': 90,\n",
       " '’': 91,\n",
       " '\\u202f': 92}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_token_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_encoder_seq_len, \n",
    "                             n_encoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_y_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인풋 데이터 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, x_txt in enumerate(x_txts):\n",
    "    \n",
    "    for t, char in enumerate(x_txt):\n",
    "        encoder_x_data[i, t, x_token_idx[char]] = 1.\n",
    "    encoder_x_data[i, t + 1:, x_token_idx[' ']] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 타겟 데이터 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, y_txt in enumerate(y_txts):\n",
    "       \n",
    "    for t, char in enumerate(y_txt):\n",
    "        decoder_x_data[i, t, y_token_idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_y_data[i, t - 1, y_token_idx[char]] = 1.\n",
    "            \n",
    "    decoder_x_data[i, t + 1:, y_token_idx[' ']] = 1.\n",
    "    decoder_y_data[i, t:, y_token_idx[' ']] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 71)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_inputs.shape)\n",
    "print(encoder_outs.shape)\n",
    "print(state_h.shape)\n",
    "print(state_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\n",
    "decoder = LSTM(latent_dim, \n",
    "                return_sequences=True, \n",
    "                return_state=True)\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,\n",
    "                             initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens, \n",
    "                                      activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 93)\n",
      "(None, None, 93)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_inputs.shape)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코더-디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 93)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 335872      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 93)     23901       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], \n",
    "              decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 적합 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 1.1878 - accuracy: 0.7248 - val_loss: 1.0634 - val_accuracy: 0.7013\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.8539 - accuracy: 0.7689 - val_loss: 0.8334 - val_accuracy: 0.7693\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.6810 - accuracy: 0.8086 - val_loss: 0.7144 - val_accuracy: 0.7937\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.5950 - accuracy: 0.8266 - val_loss: 0.6588 - val_accuracy: 0.8056\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.5449 - accuracy: 0.8408 - val_loss: 0.6247 - val_accuracy: 0.8162\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.5095 - accuracy: 0.8506 - val_loss: 0.5830 - val_accuracy: 0.8279\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.4816 - accuracy: 0.8581 - val_loss: 0.5580 - val_accuracy: 0.8356\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.4573 - accuracy: 0.8644 - val_loss: 0.5356 - val_accuracy: 0.8407\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.4361 - accuracy: 0.8702 - val_loss: 0.5274 - val_accuracy: 0.8435\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.4170 - accuracy: 0.8754 - val_loss: 0.5107 - val_accuracy: 0.8469\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.3998 - accuracy: 0.8802 - val_loss: 0.4896 - val_accuracy: 0.8537\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.3839 - accuracy: 0.8849 - val_loss: 0.4856 - val_accuracy: 0.8546\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.3690 - accuracy: 0.8890 - val_loss: 0.4738 - val_accuracy: 0.8589\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.3550 - accuracy: 0.8931 - val_loss: 0.4679 - val_accuracy: 0.8616\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.3416 - accuracy: 0.8972 - val_loss: 0.4620 - val_accuracy: 0.8619\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 33s 4ms/sample - loss: 0.3291 - accuracy: 0.9010 - val_loss: 0.4579 - val_accuracy: 0.8651\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.3173 - accuracy: 0.9046 - val_loss: 0.4513 - val_accuracy: 0.8672\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.3062 - accuracy: 0.9075 - val_loss: 0.4462 - val_accuracy: 0.8702\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.2953 - accuracy: 0.9111 - val_loss: 0.4432 - val_accuracy: 0.8709\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.2854 - accuracy: 0.9137 - val_loss: 0.4402 - val_accuracy: 0.8714\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.2757 - accuracy: 0.9167 - val_loss: 0.4371 - val_accuracy: 0.8729\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.2665 - accuracy: 0.9194 - val_loss: 0.4381 - val_accuracy: 0.8734\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.2574 - accuracy: 0.9218 - val_loss: 0.4418 - val_accuracy: 0.8725\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.2490 - accuracy: 0.9245 - val_loss: 0.4394 - val_accuracy: 0.8745\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.2408 - accuracy: 0.9270 - val_loss: 0.4391 - val_accuracy: 0.8752\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.2331 - accuracy: 0.9290 - val_loss: 0.4400 - val_accuracy: 0.8757\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.2257 - accuracy: 0.9312 - val_loss: 0.4400 - val_accuracy: 0.8762\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.2183 - accuracy: 0.9332 - val_loss: 0.4467 - val_accuracy: 0.8754\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.2117 - accuracy: 0.9354 - val_loss: 0.4449 - val_accuracy: 0.8765\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.2054 - accuracy: 0.9371 - val_loss: 0.4476 - val_accuracy: 0.8771\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.1987 - accuracy: 0.9392 - val_loss: 0.4516 - val_accuracy: 0.8761\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.1927 - accuracy: 0.9409 - val_loss: 0.4544 - val_accuracy: 0.8760\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.1870 - accuracy: 0.9427 - val_loss: 0.4503 - val_accuracy: 0.8776\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.1812 - accuracy: 0.9443 - val_loss: 0.4631 - val_accuracy: 0.8764\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1753 - accuracy: 0.9461 - val_loss: 0.4627 - val_accuracy: 0.8761\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1710 - accuracy: 0.9475 - val_loss: 0.4638 - val_accuracy: 0.8772\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.1656 - accuracy: 0.9491 - val_loss: 0.4725 - val_accuracy: 0.8761\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.1608 - accuracy: 0.9507 - val_loss: 0.4803 - val_accuracy: 0.8747\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 33s 4ms/sample - loss: 0.1565 - accuracy: 0.9519 - val_loss: 0.4797 - val_accuracy: 0.8762\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.1520 - accuracy: 0.9533 - val_loss: 0.4789 - val_accuracy: 0.8771\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1474 - accuracy: 0.9547 - val_loss: 0.4865 - val_accuracy: 0.8763\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1436 - accuracy: 0.9558 - val_loss: 0.4931 - val_accuracy: 0.8749\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1393 - accuracy: 0.9573 - val_loss: 0.4998 - val_accuracy: 0.8752\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.1361 - accuracy: 0.9579 - val_loss: 0.4973 - val_accuracy: 0.8763\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1322 - accuracy: 0.9594 - val_loss: 0.5023 - val_accuracy: 0.8755\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1288 - accuracy: 0.9602 - val_loss: 0.5059 - val_accuracy: 0.8759\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.1255 - accuracy: 0.9612 - val_loss: 0.5118 - val_accuracy: 0.8761\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.1223 - accuracy: 0.9622 - val_loss: 0.5156 - val_accuracy: 0.8761\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.1190 - accuracy: 0.9633 - val_loss: 0.5209 - val_accuracy: 0.8759\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.1158 - accuracy: 0.9640 - val_loss: 0.5281 - val_accuracy: 0.8743\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.1127 - accuracy: 0.9650 - val_loss: 0.5313 - val_accuracy: 0.8755\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.1104 - accuracy: 0.9655 - val_loss: 0.5306 - val_accuracy: 0.8750\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.1074 - accuracy: 0.9665 - val_loss: 0.5368 - val_accuracy: 0.8756\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.1048 - accuracy: 0.9673 - val_loss: 0.5451 - val_accuracy: 0.8746\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.1024 - accuracy: 0.9681 - val_loss: 0.5487 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0998 - accuracy: 0.9688 - val_loss: 0.5576 - val_accuracy: 0.8741\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0976 - accuracy: 0.9693 - val_loss: 0.5623 - val_accuracy: 0.8744\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.0953 - accuracy: 0.9700 - val_loss: 0.5619 - val_accuracy: 0.8748\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0933 - accuracy: 0.9706 - val_loss: 0.5666 - val_accuracy: 0.8749\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0905 - accuracy: 0.9715 - val_loss: 0.5709 - val_accuracy: 0.8746\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.0886 - accuracy: 0.9719 - val_loss: 0.5754 - val_accuracy: 0.8738\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.0867 - accuracy: 0.9724 - val_loss: 0.5843 - val_accuracy: 0.8738\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.0849 - accuracy: 0.9731 - val_loss: 0.5818 - val_accuracy: 0.8752\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0832 - accuracy: 0.9735 - val_loss: 0.5927 - val_accuracy: 0.8740: 0.0827 \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.0810 - accuracy: 0.9742 - val_loss: 0.5959 - val_accuracy: 0.8732\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.0796 - accuracy: 0.9743 - val_loss: 0.6008 - val_accuracy: 0.8733\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.0778 - accuracy: 0.9749 - val_loss: 0.5999 - val_accuracy: 0.8744\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.0763 - accuracy: 0.9755 - val_loss: 0.6085 - val_accuracy: 0.8738\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0745 - accuracy: 0.9759 - val_loss: 0.6110 - val_accuracy: 0.8744\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.0730 - accuracy: 0.9765 - val_loss: 0.6137 - val_accuracy: 0.8737\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.0715 - accuracy: 0.9768 - val_loss: 0.6153 - val_accuracy: 0.8741\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0697 - accuracy: 0.9774 - val_loss: 0.6254 - val_accuracy: 0.8732\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0687 - accuracy: 0.9777 - val_loss: 0.6330 - val_accuracy: 0.8735\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0672 - accuracy: 0.9781 - val_loss: 0.6382 - val_accuracy: 0.8733\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0660 - accuracy: 0.9785 - val_loss: 0.6365 - val_accuracy: 0.8736\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0647 - accuracy: 0.9789 - val_loss: 0.6412 - val_accuracy: 0.8725\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 30s 4ms/sample - loss: 0.0632 - accuracy: 0.9791 - val_loss: 0.6477 - val_accuracy: 0.8725\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.0623 - accuracy: 0.9794 - val_loss: 0.6484 - val_accuracy: 0.8728\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 33s 4ms/sample - loss: 0.0611 - accuracy: 0.9798 - val_loss: 0.6581 - val_accuracy: 0.8727\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.0595 - accuracy: 0.9801 - val_loss: 0.6594 - val_accuracy: 0.8729\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 25s 3ms/sample - loss: 0.0589 - accuracy: 0.9803 - val_loss: 0.6656 - val_accuracy: 0.8720\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0581 - accuracy: 0.9807 - val_loss: 0.6621 - val_accuracy: 0.8722\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.0569 - accuracy: 0.9811 - val_loss: 0.6651 - val_accuracy: 0.8723\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0558 - accuracy: 0.9813 - val_loss: 0.6740 - val_accuracy: 0.8722\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.6781 - val_accuracy: 0.8724\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0539 - accuracy: 0.9819 - val_loss: 0.6815 - val_accuracy: 0.8725\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0530 - accuracy: 0.9820 - val_loss: 0.6811 - val_accuracy: 0.8725\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.0518 - accuracy: 0.9826 - val_loss: 0.6849 - val_accuracy: 0.8725\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0512 - accuracy: 0.9826 - val_loss: 0.6909 - val_accuracy: 0.8711\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 27s 3ms/sample - loss: 0.0502 - accuracy: 0.9830 - val_loss: 0.6987 - val_accuracy: 0.8724\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 26s 3ms/sample - loss: 0.0497 - accuracy: 0.9831 - val_loss: 0.7013 - val_accuracy: 0.8714\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.7031 - val_accuracy: 0.8723\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 31s 4ms/sample - loss: 0.0484 - accuracy: 0.9834 - val_loss: 0.7059 - val_accuracy: 0.8728\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0476 - accuracy: 0.9835 - val_loss: 0.7075 - val_accuracy: 0.8720\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0469 - accuracy: 0.9839 - val_loss: 0.7068 - val_accuracy: 0.8715\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.0464 - accuracy: 0.9838 - val_loss: 0.7186 - val_accuracy: 0.8713\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 32s 4ms/sample - loss: 0.0460 - accuracy: 0.9843 - val_loss: 0.7137 - val_accuracy: 0.8719\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 29s 4ms/sample - loss: 0.0448 - accuracy: 0.9844 - val_loss: 0.7176 - val_accuracy: 0.8721\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 28s 4ms/sample - loss: 0.0445 - accuracy: 0.9846 - val_loss: 0.7159 - val_accuracy: 0.8722\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 28s 3ms/sample - loss: 0.0436 - accuracy: 0.9848 - val_loss: 0.7284 - val_accuracy: 0.8713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22cf8167278>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_x_data, decoder_x_data], decoder_y_data,\n",
    "          batch_size=n_batch,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론 샘플링 모형 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, \n",
    "                         decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리버스 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_x_char_idx = {}\n",
    "for char, idx in x_token_idx.items():\n",
    "    reverse_x_char_idx[idx] = char\n",
    "    \n",
    "reverse_y_char_idx ={}\n",
    "for char, idx in y_token_idx.items():\n",
    "    reverse_y_char_idx[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '!',\n",
       " 2: '$',\n",
       " 3: '%',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '0',\n",
       " 10: '1',\n",
       " 11: '2',\n",
       " 12: '3',\n",
       " 13: '5',\n",
       " 14: '6',\n",
       " 15: '7',\n",
       " 16: '8',\n",
       " 17: '9',\n",
       " 18: ':',\n",
       " 19: '?',\n",
       " 20: 'A',\n",
       " 21: 'B',\n",
       " 22: 'C',\n",
       " 23: 'D',\n",
       " 24: 'E',\n",
       " 25: 'F',\n",
       " 26: 'G',\n",
       " 27: 'H',\n",
       " 28: 'I',\n",
       " 29: 'J',\n",
       " 30: 'K',\n",
       " 31: 'L',\n",
       " 32: 'M',\n",
       " 33: 'N',\n",
       " 34: 'O',\n",
       " 35: 'P',\n",
       " 36: 'Q',\n",
       " 37: 'R',\n",
       " 38: 'S',\n",
       " 39: 'T',\n",
       " 40: 'U',\n",
       " 41: 'V',\n",
       " 42: 'W',\n",
       " 43: 'Y',\n",
       " 44: 'a',\n",
       " 45: 'b',\n",
       " 46: 'c',\n",
       " 47: 'd',\n",
       " 48: 'e',\n",
       " 49: 'f',\n",
       " 50: 'g',\n",
       " 51: 'h',\n",
       " 52: 'i',\n",
       " 53: 'j',\n",
       " 54: 'k',\n",
       " 55: 'l',\n",
       " 56: 'm',\n",
       " 57: 'n',\n",
       " 58: 'o',\n",
       " 59: 'p',\n",
       " 60: 'q',\n",
       " 61: 'r',\n",
       " 62: 's',\n",
       " 63: 't',\n",
       " 64: 'u',\n",
       " 65: 'v',\n",
       " 66: 'w',\n",
       " 67: 'x',\n",
       " 68: 'y',\n",
       " 69: 'z',\n",
       " 70: 'é'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_x_char_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\t',\n",
       " 1: '\\n',\n",
       " 2: ' ',\n",
       " 3: '!',\n",
       " 4: '$',\n",
       " 5: '%',\n",
       " 6: '&',\n",
       " 7: \"'\",\n",
       " 8: '(',\n",
       " 9: ')',\n",
       " 10: ',',\n",
       " 11: '-',\n",
       " 12: '.',\n",
       " 13: '0',\n",
       " 14: '1',\n",
       " 15: '2',\n",
       " 16: '3',\n",
       " 17: '5',\n",
       " 18: '8',\n",
       " 19: '9',\n",
       " 20: ':',\n",
       " 21: '?',\n",
       " 22: 'A',\n",
       " 23: 'B',\n",
       " 24: 'C',\n",
       " 25: 'D',\n",
       " 26: 'E',\n",
       " 27: 'F',\n",
       " 28: 'G',\n",
       " 29: 'H',\n",
       " 30: 'I',\n",
       " 31: 'J',\n",
       " 32: 'K',\n",
       " 33: 'L',\n",
       " 34: 'M',\n",
       " 35: 'N',\n",
       " 36: 'O',\n",
       " 37: 'P',\n",
       " 38: 'Q',\n",
       " 39: 'R',\n",
       " 40: 'S',\n",
       " 41: 'T',\n",
       " 42: 'U',\n",
       " 43: 'V',\n",
       " 44: 'Y',\n",
       " 45: 'a',\n",
       " 46: 'b',\n",
       " 47: 'c',\n",
       " 48: 'd',\n",
       " 49: 'e',\n",
       " 50: 'f',\n",
       " 51: 'g',\n",
       " 52: 'h',\n",
       " 53: 'i',\n",
       " 54: 'j',\n",
       " 55: 'k',\n",
       " 56: 'l',\n",
       " 57: 'm',\n",
       " 58: 'n',\n",
       " 59: 'o',\n",
       " 60: 'p',\n",
       " 61: 'q',\n",
       " 62: 'r',\n",
       " 63: 's',\n",
       " 64: 't',\n",
       " 65: 'u',\n",
       " 66: 'v',\n",
       " 67: 'x',\n",
       " 68: 'y',\n",
       " 69: 'z',\n",
       " 70: '\\xa0',\n",
       " 71: '«',\n",
       " 72: '»',\n",
       " 73: 'À',\n",
       " 74: 'Ç',\n",
       " 75: 'É',\n",
       " 76: 'Ê',\n",
       " 77: 'à',\n",
       " 78: 'â',\n",
       " 79: 'ç',\n",
       " 80: 'è',\n",
       " 81: 'é',\n",
       " 82: 'ê',\n",
       " 83: 'ë',\n",
       " 84: 'î',\n",
       " 85: 'ï',\n",
       " 86: 'ô',\n",
       " 87: 'ù',\n",
       " 88: 'û',\n",
       " 89: 'œ',\n",
       " 90: '\\u2009',\n",
       " 91: '’',\n",
       " 92: '\\u202f'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_y_char_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과값 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "    y_seq[0, 0, y_token_idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [y_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "        y_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: J’ai gagné.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Va, maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Va, maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Va, maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fled.\n",
      "Decoded sentence: J'ai fui.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: J'ai menti.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: J’ai payé.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: J'ai fait marane.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ah bon ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ah bon ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ah bon ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci.\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci.\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: On essaye.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentils !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentils !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentils !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentils !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentils !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentils !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_idx in range(100):\n",
    "    x_seq = encoder_x_data[seq_idx: seq_idx + 1]\n",
    "    decoded_sentence = decode_sequence(x_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', x_txts[seq_idx])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통합 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "n_batch = 64                            # 배치 크기\n",
    "epochs = 100                            # 학습 epochs\n",
    "latent_dim = 256                        # 단어 인코딩 축소 차원 \n",
    "n_max_sample = 10000                    # 학습 시킬 최대 샘플 수\n",
    "data_path = './data/eng-fra/fra.txt'    # 데이터 텍스트 파일 경로\n",
    "\n",
    "# 데이터 불러오기\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "# 인풋, 타겟 데이터 정리\n",
    "x_txts = []\n",
    "y_txts = []\n",
    "x_chars_uni = set()\n",
    "y_chars_uni = set()\n",
    "n_sample = min(n_max_sample, len(lines) - 1)    \n",
    "\n",
    "for line in lines[:n_sample]:\n",
    "    x_txt, y_txt, _ = line.split('\\t')\n",
    "    y_txt = '\\t' + y_txt + '\\n'\n",
    "    x_txts.append(x_txt)\n",
    "    y_txts.append(y_txt)\n",
    "    \n",
    "    for char in x_txt:\n",
    "        if char not in x_chars_uni:\n",
    "            x_chars_uni.add(char)\n",
    "    for char in y_txt:\n",
    "        if char not in y_chars_uni:\n",
    "            y_chars_uni.add(char)\n",
    "\n",
    "# 토큰 정리\n",
    "x_chars_uni = sorted(list(x_chars_uni))\n",
    "y_chars_uni = sorted(list(y_chars_uni))\n",
    "n_encoder_tokens = len(x_chars_uni)\n",
    "n_decoder_tokens = len(y_chars_uni)\n",
    "print(\"유니크 인코더 토큰 글자 수: \", n_encoder_tokens)\n",
    "print(\"유니크 디코더 토큰 글자 수: \", n_decoder_tokens)\n",
    "\n",
    "max_encoder_seq_len = 0\n",
    "for txt in x_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_encoder_seq_len = max(txt_len, \n",
    "                              max_encoder_seq_len)\n",
    "print(\"인코더 문장내 최대 문자 수: \", max_encoder_seq_len)\n",
    "\n",
    "max_decoder_seq_len = 0\n",
    "for txt in y_txts:\n",
    "    txt_len = len(txt)\n",
    "    max_decoder_seq_len = max(txt_len, \n",
    "                              max_decoder_seq_len)\n",
    "print(\"디코더 문장내 최대 문자 수: \", max_decoder_seq_len)\n",
    "\n",
    "# 토큰 인덱스\n",
    "x_token_idx = {}\n",
    "for idx, char in enumerate(x_chars_uni):\n",
    "    x_token_idx[char] = idx\n",
    "    \n",
    "y_token_idx ={}\n",
    "for idx, char in enumerate(y_chars_uni):\n",
    "    y_token_idx[char] = idx\n",
    "    \n",
    "# 영행렬 만들기\n",
    "encoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_encoder_seq_len, \n",
    "                             n_encoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_x_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "decoder_y_data = np.zeros(\n",
    "                            (len(x_txts), \n",
    "                             max_decoder_seq_len, \n",
    "                             n_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "# 인풋 데이터 행렬\n",
    "for i, x_txt in enumerate(x_txts):\n",
    "    \n",
    "    for t, char in enumerate(x_txt):\n",
    "        encoder_x_data[i, t, x_token_idx[char]] = 1.\n",
    "    encoder_x_data[i, t + 1:, x_token_idx[' ']] = 1.\n",
    "\n",
    "# 타겟 데이터 행렬\n",
    "for i, y_txt in enumerate(y_txts):\n",
    "       \n",
    "    for t, char in enumerate(y_txt):\n",
    "        decoder_x_data[i, t, y_token_idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_y_data[i, t - 1, y_token_idx[char]] = 1.\n",
    "            \n",
    "    decoder_x_data[i, t + 1:, y_token_idx[' ']] = 1.\n",
    "    decoder_y_data[i, t:, y_token_idx[' ']] = 1.\n",
    "    \n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens))\n",
    "decoder = LSTM(latent_dim, \n",
    "                return_sequences=True, \n",
    "                return_state=True)\n",
    "decoder_outs, _, _ = decoder(decoder_inputs,\n",
    "                             initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(n_decoder_tokens, \n",
    "                                      activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outs)\n",
    "\n",
    "# 인코더 디코더\n",
    "model = Model([encoder_inputs, decoder_inputs], \n",
    "              decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "# 모형 컴파일\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습\n",
    "model.fit([encoder_x_data, decoder_x_data], decoder_y_data,\n",
    "          batch_size=n_batch,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 추론 모형 생성\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, \n",
    "                         decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# 리버스 인덱스\n",
    "reverse_x_char_idx = {}\n",
    "for char, idx in x_token_idx.items():\n",
    "    reverse_x_char_idx[idx] = char\n",
    "    \n",
    "reverse_y_char_idx ={}\n",
    "for char, idx in y_token_idx.items():\n",
    "    reverse_y_char_idx[idx] = char\n",
    "\n",
    "# 결과값 디코딩\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "    y_seq[0, 0, y_token_idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [y_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_y_char_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        y_seq = np.zeros((1, 1, n_decoder_tokens))\n",
    "        y_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# 최종 결과 확인\n",
    "for seq_idx in range(100):\n",
    "    x_seq = encoder_x_data[seq_idx: seq_idx + 1]\n",
    "    decoded_sentence = decode_sequence(x_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', x_txts[seq_idx])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
